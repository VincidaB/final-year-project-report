\documentclass[11pt]{article}
\usepackage[T1]{fontenc}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\pagestyle{fancy}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\usepackage{glossaries}
\usepackage{subfigure}
%\usepackage{dot2texi}

\makenoidxglossaries



\loadglsentries{lexique}


\usepackage[table,xcdraw, x11names, svgnames, rgb]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{eso-pic}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning, snakes}
\usepackage{float}
\usepackage{textcomp}
\usepackage{soul}
\usepackage{listings}
\newcommand{\hilight}{\makebox[0pt][s]{\color{green!50}\rule[-3.6pt]{1.0\linewidth}{12pt}}}

\usepackage{tikz}
\usepackage{listofitems} % for \readlist to create arrays


\tikzstyle{mynode}=[thick, draw=blue, fill=blue!20,circle, minimum size=22]

\lstdefinestyle{xmlStyle}{
    language=XML,
    basicstyle=\ttfamily\small,
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=10pt,
    backgroundcolor=\color{white},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    frame=single,
    tabsize=2,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    title=\lstname,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
}

\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\lstdefinestyle{command}{
  backgroundcolor=\color{white},
  basicstyle=\ttfamily\color{black},
  keywordstyle=\color{blue},
  commentstyle=\color{mygray},
  stringstyle=\color{red},
  showstringspaces=false,
  upquote=true,
  morekeywords={sudo, ls, cd, mv, cp, rm, mkdir, chmod, chown, grep, find},
  captionpos=b,
  frame=single,
  numbers=left,
  numberstyle=\tiny\color{mygray},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2,
  keepspaces=true,
  caption={Linux Command},
  label=command,
}
\lstdefinestyle{bashstyle}{
  backgroundcolor=\color{white},
  basicstyle=\ttfamily\color{black},
  keywordstyle=\color{blue},
  commentstyle=\color{mygray},
  stringstyle=\color{red},
  showstringspaces=false,
  upquote=true,
  morekeywords={sudo, ls, cd, mv, cp, rm, mkdir, chmod, chown, grep, find},
  captionpos=b,
  frame=single,
  numbers=left,
  numberstyle=\tiny\color{mygray},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2,
  keepspaces=true,
  caption={Script bash},
  label=command,
}

\lstdefinestyle{cstyle}{
    language=C,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{green!40!black},
    stringstyle=\color{red},
    identifierstyle=\color{black},
    captionpos=b,
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    showstringspaces=false,
    tabsize=4,
    morekeywords={int, char, void, if, else, while, for, return, typedef, struct, include},
    columns=flexible
}

\lstdefinestyle{makefilestyle}{
    language=make,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{green!40!black},
    captionpos=b,
    stringstyle=\color{red},
    identifierstyle=\color{black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    showstringspaces=false,
    tabsize=4,
    morekeywords={ifeq, endif, else, ifdef, ifndef, define, endef, export, unexport, obj},
    columns=flexible
}
\lstdefinestyle{config}{
    language=make,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{green!40!black},
    captionpos=b,
    stringstyle=\color{red},
    identifierstyle=\color{black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    showstringspaces=false,
    tabsize=4,
    morekeywords={ifeq, endif, else, ifdef, ifndef, define, endef, export, unexport, obj},
    columns=flexible
}

\bibliographystyle{plain} % We choose the "plain" reference style

\renewcommand{\epsilon}{\varepsilon} 
\renewcommand{\phi}{\varphi} 
\title{Final Year Project Report\vspace{10pt}\\
**************************************************\\
Multi Agent Scene Exploration and Mapping \\
for Continuous Digital Twin Creation\vspace{10pt}\\
**************************************************}
\author{BELPOIS Vincent \\ Under the supervision of Dr. Ivan \textsc{Mutis}}

\begin{document}

\date{2024}
\maketitle
\thispagestyle{empty}

\vspace{10mm}

\begin{center}
\includegraphics[width = 6cm]{Images/logo_ensma.png}
\end{center}
\vspace{2cm}
\begin{center}
    \includegraphics[width = 8cm]{Images/IIT_Logo_stack_186_blk.png}
\end{center}
\vspace{1cm}
\begin{center}
    \includegraphics[width = 6cm]{Images/logo_iconsense_bloack_text.png}
\end{center}
\newpage
\thispagestyle{empty}
\mbox{}




\AddToShipoutPictureBG{%
\put(15,7){\includegraphics[scale = 0.02]{Images/logo_ensma.png}}
\put(480,10){\includegraphics[width = 100pt]{Images/IIT_Logo_stack_186_blk.png}}
\put(400,14.9){\includegraphics[width = 75pt]{Images/logo_iconsense_bloack_text.png}}
}



\newpage
\section*{Acknowledgments}

    Acknowledgments.
    
\newpage
\thispagestyle{empty}
\mbox{}
\newpage
\thispagestyle{empty}
{\small \tableofcontents}

\newpage
\thispagestyle{empty}
\mbox{}
\newpage

\section{Introduction}

\begin{tikzpicture}[x=2.2cm,y=1.4cm]
    \readlist\Nnod{4,5,4,5,3,2} % number of nodes per layer
    % \Nnodlen = length of \Nnod (i.e. total number of layers)
    % \Nnod[1] = element (number of nodes) at index 1
    \foreachitem \N \in \Nnod{ % loop over layers
        % \N     = current element in this iteration (i.e. number of nodes for this layer)
        % \Ncnt  = index of current layer in this iteration
        \foreach \i [evaluate={\x=\Ncnt; \y=\N/2-\i+0.5; \prev=int(\Ncnt-1);}] in {1,...,\N}{ % loop over nodes
        \node[mynode] (N\Ncnt-\i) at (\x,\y) {};
        \ifnum\Ncnt>1 % connect to previous layer
            \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
            \draw[thick] (N\prev-\j) -- (N\Ncnt-\i); % connect arrows directly
            }
        \fi % else: nothing to connect first layer
        }
    }
    \end{tikzpicture}

    $$
    \int_\alpha^\beta f(x) dx
    $$
    \cite{xu2022fast}


\newpage

\section[Setting up the physical agents]{Setting up the physical agents}
\label{section:big title}%label to reference section

        Three platforms were used in this project: a six-wheeled platform, a quadcopter, and a quadruped platform. Each platform was chosen for its specific characteristics, with the goal of having a multi-agent system that could explore and map a scene. The six-wheeled platform was chosen for its stability and ability to carry heavy loads. The quadcopter was chosen for its ability to fly and provide a bird's-eye view of the scene. The quadruped platform was chosen for its ability to climb stairs and navigate bumpy terrain.

    \subsection{Six wheeled platform setup}

        The six-wheeled platform was chosen for its stability and its ability to carry heavy loads. The intent was to have it carry a robotic arm for other projects. Some previous work had already been done on the platform but was apparently unsuccessful. These earlier attempts had left the platform in a partially modified state, requiring a comprehensive reassessment and redesign of both the mechanical and electrical systems. Despite these setbacks, the robust chassis of the six-wheeled platform still presented an excellent foundation for our project, offering the potential for further development.

        \subsubsection{Mechanical modifications}
        As it arrived, the six-wheeled platform only consisted of a stainless steel chassis, six DC motors, and wheels. The platform required several mechanical modifications to accommodate the necessary components for autonomous operation. Specifically, it needed a mount for a LIDAR sensor and an embedded computer on its top surface (see Figure \ref{fig:LIDAR_mount}). Additionally, a mounting solution for the motor drivers inside the chassis was essential (see Figure \ref{fig:motor_driver_mount}). These modifications were designed and implemented to ensure proper integration of all components while maintaining the structural integrity of the platform (see Figure \ref{fig:full_cad_model}).

        \begin{figure}[htbp]
            \centering
            LIDAR MOUNT CAD
            %\includegraphics[width=0.8\textwidth]{images/lidar_mount_cad.png}
            \caption{Drawing of CAD model of the LIDAR sensor and embedded computer mount}
            \label{fig:LIDAR_mount}
        \end{figure}
        
        \begin{figure}[htbp]
            \centering
            %MOTOR DRIVER MOUNT CAD
            \includegraphics[width=0.3\textwidth]{Images/Motor_driver_holder.pdf}
            \caption{Drawing of CAD model of the motor driver mounting solution}
            \label{fig:motor_driver_mount}
        \end{figure}
        
        \begin{figure}[htbp]
            \centering
            \includegraphics[width=0.8\textwidth]{Images/RoverV1.pdf}
            \caption{Drawing of CAD model of the modified six-wheeled platform's top}
            \label{fig:full_cad_model}
        \end{figure}

        \subsubsection{Electronics architecture}
            The essential electronic components needed to get the platform running were mainly DC motor drivers to drive the motors, a LIDAR sensor and an embedded computer.
            
            Difficulties were encountered when trying to use the drivers someone else tried beforehand as they were under powered : at stall, the motors required around 5 amps, as measured with a bench top power supply, and the drivers I was trying to use were only capable of delivering 2 amps per channel or a total of 4 amps when combining outputs. The drivers in question were the based on the LN298N which were in terms replaced by the 7A dual motor drivers from DFRobot. A physical comparison can be seen in \ref{fig:drivers_comparison}.
        
            
            \begin{figure}[h]
                \centering
                %INSERT (a) (b) PICTURE OF BOTH DRIVERS
                \includegraphics[width=0.3\textwidth]{Images/olddrivers.jpg}
                \includegraphics[width=0.4\textwidth]{Images/newdrivers.jpg}
                \caption{L298N (a) and DFRobot 7A dual DC Motor Driver (b)}
                \label{fig:drivers_comparison}
            \end{figure}

            The three motor drivers were connected to a microcontroller. The connection can be seen in \ref{fig:driver_to_pico}.
            I chose to use a Raspberry Pi Pico microcontroller for its many outputs, totaling 26 general purpose input outputs (GPIO). Each driver required 6 control signals or 3 per motor: two signals are used to control the direction of the motor according to Table \ref{tab:motor_control}, while the third signal's duty cycle determines the speed.

            A radio control (RC) receiver was also connected to interrupt capable GPIOs of the microcontroller to be able to control the platform manually. Three channels of the RC receiver were used to control the speed, the direction and the mode of the platform. The mode refers to whether or not the platform is in manual control or in autonomous mode and is connected to channel 5 of the radio which has a two-way switch.

            Finally, the Pico is connected to an Nvidia Jetson Orin single board computer (SBC) via USB. This connection is used both to reprogram the Pico, and to send speed and direction commands to each motor via a serial communication. 

            Not including the power distribution and regulation system, \ref{fig:overall_electical_system} shows the electrical connections of these components on the modified six-wheeled platform.


            \begin{table}[h!]
                \centering
                \begin{tabular}{|c|c|c|c|}
                \hline
                IN1 & IN2 & ENA/ENB & Motor1/2 Behavior \\ \hline
                0   & 0   & x       & Stop (brake)      \\ \hline
                1   & 1   & x       & Vacant            \\ \hline
                1   & 0   & 1       & Forward 100\%     \\ \hline
                0   & 1   & 1       & Reverse 100\%     \\ \hline
                1   & 0   & PWM     & Forward at PWM speed \\ \hline
                0   & 1   & PWM     & Reverse at PWM speed \\ \hline
                \end{tabular}
                \caption{Motor control signal table}
                \label{tab:motor_control}
            \end{table}

            %! Add a reference to this figure !!
            \begin{figure}[H]
                \centering
                %OVERALL ELECTRICAL SYSTEM (excluding power)
                
                \includegraphics[width=0.8\textwidth]{Images/PFE-Page-2.drawio.png}
                \caption{Overall electrical system, excluding power distribution and regulation}
                \label{fig:overall_electical_system}
            \end{figure}


            
            
            \begin{figure}[h]
                \centering
                \color{red}
                Driver to PICO connection diagram
                \label{fig:driver_to_pico}
            \end{figure}

        
        
        \subsubsection{Software architecture}

        %How the software on the jetson communicates with the microcontroller and how it communicates with the LIDAR. The choice of the odometry algorithm will be explain in another section (master, comparison of multiple algo)

        The robot operating system (ROS) was chosen as the software framework for the platform running on the Jetson Orin embedded computer. Specifically, ROS2 Humble Hawksbill was selected due to its extensive package availability and compatibility with the Jetson Orin's hardware. Indeed, I was made aware of the struggles of another researcher running a ROS based robot on an Nvidia Jetson Nano and how Ubuntu, and ROS version mismatch may bring problems. 
        
        This version of ROS2 provides a robust and flexible framework for developing and integrating various components of the platform, including sensor processing, navigation, and control. For real-time critical tasks such as motor control and RC radio interrupts, the Raspberry Pi Pico microcontroller was utilized, leveraging its ability to handle low-level, time-sensitive operations. The microcontroller's firmware was developed from the ground up by myself, guaranteeing reliable execution of motor control and interrupt handling tasks. By combining the strengths of ROS2 on the Jetson Orin with the real-time capabilities of the Raspberry Pi Pico, the platform achieves a robust and efficient software architecture that enables seamless integration of autonomous navigation, sensor processing, and manual control.

        %! Add a reference to this figure !!
        \begin{figure}[H]
            \centering
            %Software stack, from jetpack to ros2 to the nodes to the microcontroller
            \includegraphics[width=0.8\textwidth]{Images/Software architecture rover.drawio.png}
            \caption{Software architecture}
            \label{fig:SW_architecture}
        \end{figure}


        \subsubsection{ROS 2 setup}
        
        % What nodes did I create, explain the setup with nav2 working.
        % TODO : Write a small paragraph in order to explain how ROS2 is setup and what external packages we need

        \subsubsection{Issues encountered}
            
        %Explain the state reached (nav2 navigation kinda working) and why the other platform was created

        The 6-wheeled rover, with its stainless steel frame and weak DC motors lacking encoders, was not the most suitable platform for the task of autonomous navigation. The lack of encoders on the motors made odometry calculations based on the LIDAR and inertial measurement unit unreliable. An attempt was made to do closed-loop control with the aforementioned odometry, but the noise and the lack of per-wheel odometry made it impossible to have a stable and reliable control. I then decided to create a new platform using closed-loop stepper motors, also called servos. Their closed-loop control and high torque at low speed would make them ideal for slow movements, thereby making the task of autonomous navigation feasible.

        %! Add a reference to this figure !!
        \begin{figure}[h!]
            \centering 
            \subfigure[Version 1 of the rover]{\includegraphics[width=0.4\textwidth, angle=270]{Images/roverv1closed.jpg}}
            \subfigure[Version 2 of the rover]{\includegraphics[width=0.4\textwidth, angle=270]{Images/roverv2openened.jpg}}       
            \caption{Rover version 1 and 2}
        \end{figure}


        %Explain how the chassis was designed to fit the new closed loop, motors

    \subsection{Quadcopter setup}
            
            % Explain the platform's starting point and the goal
            % Explain the different drones that were at my disposal and which one was selected for our application (the one that could easel;y carry the LIDAR, jetson while not being too big)

            One of the agents in our multi-agent system is a quadcopter, chosen for its ability to provide aerial perspective and navigate in three-dimensional space. After evaluating several drone options available to us, we selected a model that struck an optimal balance between payload capacity and size. This drone was capable of easily carrying the Livox Mid-360 LIDAR sensor and the Nvidia Jetson Orin embedded computer, while still maintaining a compact form factor suitable for indoor and outdoor operations.

            The quadcopter platform required several modifications to integrate our specific sensor suite and computational hardware.

            \subsubsection{Mechanical modifications}

                I designed and 3D printed new landing legs that fit on the arms of the quadcopter. Those landing legs were designed to increase the landing stability, which I noticed was a problem in the manual flight I performed, and to reduce the blind spots of the LIDAR which was to be placed in the center of the underside of the drone.

                I also took the opportunity to redesign the battery mounting mechanism which was bulky, heavy, and suitable to only one size of battery to one that is much simpler and uses Velcro straps as can be seen in Figure \ref{fig:landing_legs}. This resulted in a saving of \color{red} XXX grams \color{black} and thus increased the flight time of the drone. 
                

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.6\textwidth]{Images/BatteryTrayDrawing.pdf}
                    \caption{Drawing of CAD model of new battery tray}
                    \label{fig:landing_legs}
                \end{figure}    
            \subsubsection{Electronics architecture}

                % Similarly to the rover, the jetson is still connected to the LIDAR via ethernet 
                % The drone is controlled by a PLACEHOLDER_CONTROLLER, which is connected to the jetson via USB

                To keep the architecture similar to the one used on the wheeled robot, the quadcopter was also equipped with an Nvidia Jetson Orin embedded computer and a Livox Mid-360 LIDAR. The computer and the LIDAR get their power from the drone's battery, which is regulated to 19V and 12V respectively.

            \subsubsection{Software architecture and setup}
                
                % don't know what planner to use, something complicated or just a grid coverage planner
        
            \subsubsection{Issues encountered}
            
    \subsection{Quadruped platform setup}

        A quadruped was chosen as the third platform as it can navigate rough terrain and climb stairs. The platform chosen was the Unitree GO2, a quadruped robot with a 3D LIDAR and an embedded computer.

        \subsubsection{Compute backpack}

        Even though the Unitree GO2 has a LIDAR of its own and an embedded computer based on the Rockchip RK3588S inside, we decided to add an additional LIDAR on top of the platform and as such, we needed an additional embedded computer.

        As is the case with the wheeled platform and the drone, we chose to use a Livox Mid-360 LIDAR and a Nvidia Jetson Orin embedded computer. Even though the quadruped already carries a 3D LIDAR, we chose to use a Mid-360 as the ladder produces around ten times more points per second (21600 points per second for the Unitree L1 LIDAR against the 200000 points per second of the Mid-360).

        \begin{figure}[H]
            \centering
            \begin{minipage}{0.45\textwidth}
                The compute backpack also provides access to a power port with a connector Amass XT-30 and a gigabit Ethernet port on the robot that are usually covered by a plastic cover. Those ports are covered by the handle of the GO2 as can be seen in the figure \ref{fig:handle_cover}.
            \end{minipage}%
            \hfill
            \begin{minipage}{0.5\textwidth}
                \centering
                \includegraphics[width=0.75\textwidth]{Images/PortsWithCover.jpg}
                \caption{Handle and covered connectors}
                \label{fig:handle_cover}
            \end{minipage}
        \end{figure}
        To fit the Nvidia Jetson, the LIDAR, and the power regulators needed for both of them, the top part of the robot was scanned, and a new top part was designed and 3D printed. The CAD model of top part and the scan can be seen in Figure \ref{fig:scanner_and_cad}. This holds the LIDAR on the back of the robot to be sure to cover the grounds during the scanning process. The angle was determined as to not have any of the robot itself in the field of view of the LIDAR. Additionally, the compute backpack was designed to accommodate other experiments like mounting a robotic arm on the robot. For that purpose, all four sides figure threaded mounting holes.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.3\textwidth]{Images/ScanGO2Top.pdf}
            \includegraphics[width=0.6\textwidth]{Images/ComputeBackpack.pdf}
            \caption{Scanned top of GO2 robot and drawing of CAD model of the compute backpack }
            \label{fig:scanner_and_cad}
        \end{figure}




        \begin{figure}[H]
            \centering
            \color{red}
            IMAGE OF COMPUTE BACKPACK
        \end{figure}


        
        \subsubsection{Communication with the quadruped}

        % explain how we use webRTC to communicate with the robot (explain whatit is and how it works)
        % explain how we could use the cyclondeDDS because we have access to the newly jailbreaked robot
        
    \subsection{Issues encountered}
        
        %Big battery issue, reverse engineering of the battery, jailbreaking of the robot
        As delivered, the robot was not functional as the battery was discharged beyond the point of no return. The battery was disassembled (see Figure \ref{fig:disassembled_battery}) and partly reverse engineered to understand the problem. Even though the battery was not repairable, the reverse engineering allowed us to understand the battery's BMS and how it communicated with the robot. In Table \ref{table:battery_protocol} it can be seen that I was able to partly reverse engineer the battery communication protocol. This was shared with others from the open source community actively trying to design a replacement motherboard for the robot. 
        
        The battery was then replaced by a new one and the robot was jailbroken to allow for more control over the robot.
        

        \begin{figure}[h]
            \centering
            \subfigure[Disassembled battery]{\includegraphics[width=0.4\textwidth]{Images/teardown_battery.jpg}}
            \subfigure[Battery Management System (BMS)]{\includegraphics[width=0.44\textwidth]{Images/BMS.jpg}}
            \caption{Disassembled battery and Battery Management System (BMS)}
            \label{fig:disassembled_battery}
        \end{figure}

        \begin{table}
            \centering
            \begin{tabular}{|c|c|c|c|}
                \hline
                Byte & Data & Conversion & Comment \\
                \hline
                0 & EF FE & -4098 & start of battery message \\
                2 & 3C 03 & 15363 & start of battery message \\
                4 & 0F 00 & 3840 & cell 8 voltage in mV \\
                6 & 0F 01 & 3841 & cell 7 voltage in mV \\
                8 & 0F 01 & 3841 & cell 6 voltage in mV \\
                10 & 0F 03 & 3843 & cell 5 voltage in mV \\
                12 & 0F 01 & 3841 & cell 4 voltage in mV \\
                14 & 0E FF & 3839 & cell 3 voltage in mV \\
                16 & 0F 01 & 3841 & cell 2 voltage in mV \\
                18 & 0F 01 & 3841 & cell 1 voltage in mV \\
                20 & 77 FB & 30715 & battery total voltage in mV \\
                22 & 01 29 & 297 &  \\
                24 & FF F8 & -8 &  \\
                26 & FF FF & -1 &  \\
                28 & 64 00 & 100 & SoC in percent \\
                30 & 09 BF & 2495 & temperature 24.95 C \\
                32 & 09 C6 & 2502 & temperature 25.02 C \\
                34 & 09 A4 & 2468 & temperature 24.68 C \\
                36 & 09 A8 & 2472 & temperature 24.72 C \\
                38 & 00 00 & 0 &  \\
                40 & 57 70 & 22384 &  \\
                42 & 09 01 & 2305 & number of charge or discharge cycles \\
                44 & 0D 06 & 333 &  \\
                46 & 00 00 & 0 &  \\
                48 & 00 00 & 0 &  \\
                50 & 00 00 & 0 &  \\
                52 & 00 FF & 255 &  \\
                54 & 00 00 & 0 &  \\
                56 & 47 80 & 18304 & crc32 \\
                58 & 7E 9D & 32413 & crc32 \\
                \hline
                \end{tabular}
                \label{table:battery_protocol}
            \caption{Partly decoded battery protocol}
        \end{table}


        One of the other issues encountered happened after a software update of the robot. We weren't able to connect via SSH to the Linux computer inside the robot. This required many days of reverse engineering of the new firmware with the help of other people online.

        The update had removed a vulnerable update mechanism that previously allowed us to gain a root shell to the robot. With that shell, we were then able to change the password of the robot and gain access to the robot via SSH. The new firmware had also prevented us from using the Ethernet port of the robot to communicate to the robot using the CycloneDDS which is one of the Data Distribution Services that ROS can use. This functionality of the robot is usually reserved for the educational version of the GO2, but the company doesn't offer any upgrade from the standard version to the educational version.

        As such, I helped the open source community to design a new unlocking method. For reference, in the previous version, only a version number in a single file needed to be changed from a 2 to a 4 ti unlock every functionality of the robot. The new update brought many new challenges as every binary was not heavily obfuscated and had increased in size by a factor of 10, every file's checksum was now verified by a yet unknown program, no debugger could be attached as every binary checked their parent process, and many other obfuscation techniques were used.

        The anti debugger technique was quickly bypassed as only the name of the parent process was checked against a list of names, \texttt{gdb-server} not being part of this list made it easy to debug the binaries. Every binary also checked for the presence of a \texttt{tracepid} value, which was solved at first with kernel module that hides the presence of the \texttt{tracepid} value. Other threads were also created to continue monitoring those values in the binaries, but patching the \texttt{pthread\_create} instructions to \texttt{MOV X0, \#0} solved that problem.

        Once the location of the checksum check was identified in a binary, the instruction responsible for the comparison was patched to always return true : the previous instruction \texttt{BL <compare>} was replaced by \texttt{MOV X0, \#0}.

        Finally, the version check was bypassed by consolidating two instruction in one, to in terms have room to set the value of the register \texttt{X0} to 4 as can be seen in the table \ref{tab:patched_instructions}.
        

        % Please add the following required packages to your document preamble:
        % \usepackage[table,xcdraw]{xcolor}
        % Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
        \begin{table}[]
            \centering
            \mbox{}\clap{
            \begin{tabular}{|l|l|l|l|l|}
            \hline
            \textbf{Address} & \textbf{Original value} & \textbf{Original Instruction}      & \textbf{Patched value} & \textbf{Patched instruction}       \\ \hline
            \texttt{565BD80} & \texttt{E0 7B BF A9}   & \texttt{stp     x0, x30, {[}sp,\#-0x10{]}!} & \texttt{80 00 80 D2}            & \texttt{mov x0, \#4}                    \\ \hline
                                & \texttt{63 4D 00 94}   & \texttt{bl      sub\_566F310              } & \texttt{E0 7B BF A9}            & \texttt{stp x0, x30, {[}sp,\#-0x10{]}!} \\ \hline
                                & \texttt{84 00 00 18}   & \texttt{ldr     w4, \#0x565BD98           } & \texttt{63 4D 00 94}            & \texttt{bl sub\_566F310}                \\ \hline
                                & \texttt{84 7C 40 93}   & \texttt{sxtw    x4, w4                    } & \texttt{64 00 00 98}            & \texttt{ldrsw X4, \#0x565BD98}          \\ \hline
            \end{tabular}
            }
            \label{tab:patched_instructions}    
            \caption{Patched instructions for version check}
        \end{table}




    \newpage

\section{Mapping, planning, and exploration algorithms}

    There are three main components to the software architecture of the multi-agent system: mapping, planning, and exploration. Mapping first involves knowing where the robot is and how it is moving, or it's odometry. Planning is often divided in two scales, a local one where we consider what movements the robot is capable of doing, a global one that aims to find the best path to a goal. Exploration uses the two former process as to determine where the robot should go to maximize the information gathered.

    \subsection{Mapping}

        Mapping is the action of aligning the robot's acquired LIDAR scans in relation to the initial pose. For that, one or multiple methods of odometry are needed. 
        \subsubsection{Introduction to SLAM`}'
        SLAM or Simultaneous Location And Mapping is the action of using a scanner, in our case a LIDAR, for both the construction of a map, and utilizing this map to infer our position. In 2D, algorithms like Monte Carlo based particle filters \cite{fox1999monte}, or graph based approaches \cite{macenski2021slam} are used and have not been lately improved. In 3D, the problem is much more complex and the algorithms are much more computationally expensive.  


        
        
        \subsubsection{LIDAR inertial odometry}
        
        LIDARs are often paired with Inertial Measurement Units (IMUs) as algorithms can be conceived to make use of both the LIDAR and the IMU for odometry. Registration of features of the last scan of the LIDAR with previous ones provide long term accuracy and minimize potential drift, while the IMU provide a high rate of movement data that can be used to both facilitate the alignment, as well as to undistort the scans that were captured during fast movements.
        
        As the Livox Mid-360 also carries an IMU, this is the reason why I turned myself to those kinds of algorithms for the odometry. 
        
        One such algorithm is FAST-LIO2 \cite{xu2022fast}, which I chose for being close to the state of the art and having a version available that was compatible with the MID-360 LIDAR. A more detailed comparison of the different LIDAR inertial odometry algorithms will be presented in the section \ref{section:etatArt}.
        
        
        %! add reference to this figure
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.95\textwidth]{Images/overview_fastlio2.png}
            \caption{FAST-LIO2 algorithm}
        \end{figure}
        
        \subsubsection{Comparison of LIDAR inertial odometry algorithms}

        The comparison of LIDAR inertial odometry algorithms was chosen as it is the data that is available to us. LIDAR inertial odometry algorithms can be divided in two categories based on how the LIDAR and IMU data are fused : loosely coupled and tightly coupled.

        Loosely coupled methods estimate the state of each sensor independently, combines the measurement with weights, and then determines the position of the robot. Those methods are useful when both sensors are independent and changes to them want to be tested. The weights can also be changed depending on the sensors used and on their accuracy and noise, or dynamically if a sensor fails or malfunctions. However, we have a LIDAR package that integrates an IMU, making the aforementioned benefits not relevant to us.

        Tightly coupled methods, on the other hand, use the measurements of all sensors to estimate the robot's position. This can mean using the IMU readings to pre process the LIDAR measurements. Those methods however require a more complex algorithm and can be more computationally expensive. They can also be more prone to failure as the failure of one sensor entails the failure of the odometry system.


        LOAM (LIDAR Odometry And Mapping) \cite{lee2024lidar} was the first loosely coupled method to be proposed and offered low computational requirements. It used the IMU measurements to estimate velocity while the LIDAR measurement used the velocity estimation for undistortion to offer better position estimation. LeGo-LOAM \cite{shan2018lego} improved on the works of LOAM by improving the performance of the pose estimation and by adding ground segmentation, making the method more robust in ground based applications.

        As for tightly coupled methods to be proposed in the Zebedee paper \cite{bosse2012zebedee} showed the great accuracy the tightly coupled methods can offer. It demonstrated stable mapping of a scene by placing a 2D LIDAR on a spring mobile base. LIPS \cite{geneva2018lips} formalized the closest point plane representation and added IMU preintegration for more accurate pose estimation. Ye et al. \cite{ye2019tightly} added joint optimization of the LIDAR and IMU measurements, improving the performance in high drift scenarios in long term mapping. This paper also added a rotation-contrained refinement which improves the consistency of the global map.
        FAST-LIO \cite{xu2021fast} greatly improved the performance of previous works by introducing the use of a tightly coupled iterated Kalman Filter. The high computing efficiency of the proposed gain calculation formula improves the performance, making this method interesting for real time applications on embedded systems. FAST-LIO2 \cite{xu2022fast} improved on FAST-LIO by getting rid of the feature extraction step and replacing it registration to the global map. It also improved performance by switching to an incrementally updated kd-tree data structure for map storage. 
        POINT-LIO \cite{he2023point} employs a different method to the previously mentioned algorithms by using a point-wise position estimation providing a much higher frequency of odometry updates.  

        \label{section:etatArt}


        \subsubsection{Terrain analysis}
        \label{section:terrain_analysis}
        % explain the two terrain analysis we do on the acquired point cloud

        To determine the navigable and non-navigable areas, the aligned point cloud gathered by the LIDAR and the odometry are analyzed. The first analysis is one at a closer range, while the other has a longer range and a longer decay time. The topics that the two analysis subscribe and publish to are shown in Figure \ref{fig:terrain_analysis}. The extended range analysis is used by some planners to determine the best path to take while the close range analysis is used to determine the immediate surroundings of the robot as is showcased in Figure \ref{fig:terrain_analysis}.

        % The following figure shows that both terrain_analysis and terrain_analysis_ext 
        % subscribe to /odom, /registered_scan and publish /terrain_map and /terrain_map_ext respectively.
        \begin{figure}[H]
                \centering
                \begin{tikzpicture}[node distance=2cm, auto]
                        % Nodes
                        \node (odom) [draw, rectangle, minimum width=2.5cm, minimum height=1cm] {/odom};
                        \node (scan) [draw, rectangle, minimum width=2.5cm, minimum height=1cm, below of=odom] {/registered\_scan};
                        \node (ta) [draw, ellipse, minimum width=2.5cm, minimum height=1.25cm, right of=odom, node distance=5.5cm] {terrain\_analysis};
                        \node (ta_ext) [draw, ellipse, minimum width=5.5cm, minimum height=1.25cm, below of=ta] {terrain\_analysis\_ext};
                        \node (terrain_map) [draw, rectangle, minimum width=2.5cm, minimum height=1cm, right of=ta, node distance=5.5cm] {/terrain\_map};
                        \node (terrain_map_ext) [draw, rectangle, minimum width=2.5cm, minimum height=1cm, below of=terrain_map] {/terrain\_map\_ext};

                        % Arrows
                        \draw[->] (odom) -- (ta);
                        \draw[->] (scan) -- (ta);
                        \draw[->] (odom) -- (ta_ext);
                        \draw[->] (scan) -- (ta_ext);
                        \draw[->] (ta) -- (terrain_map);
                        \draw[->] (ta_ext) -- (terrain_map_ext);
                \end{tikzpicture}
                \caption{Subscriptions and publications of terrain\_analysis and terrain\_analysis\_ext}
                \label{fig:terrain_analysis}
        \end{figure}

        % TODO explain the algo used
        
        % TODO add a comparaison of the two obtained opontclouds

        %! add reference to this figure
        \begin{figure}[H]
            \centering
            \subfigure[Close range terrain analysis]{\includegraphics[width=0.45\textwidth]{Images/terrainAnalysis.png}}
            \subfigure[Extended terrain analysis]{\includegraphics[width=0.45\textwidth]{Images/terrainAnalysisExt2.png}}
            \caption{Comparison of close range and extended range terrain analysis}
        \end{figure}

        This terrain analysis paradigm is derived from the works of Ji Shang et al. on fast likelihood-based collision avoidance \cite{zhang2020falco}, as is the local path planning that will be described in section \ref{section:local_path_planning}.
        

    \subsection{Planning}
        The problem of path planning in 2D is a well known problem in robotics and has been solved many times. However, the problem of path planning in 3D is much more complex that has only recently found efficient solutions. 

        \subsubsection{2D path planning}
        % Using nav2 as a base and how we convert from a 3D point cloud to a 2D one 
        As a first test of the wheeled and quadruped platform, a simple 2D navigation was put in place to showcase the navigation of the platforms on flat ground. Nav2 \cite{macenski2020marathon2} was used with a simple Smac planner \cite{macenski2024smac} to navigate the platforms. The 3D point cloud was converted to a 2D one by taking only a range of the points relative to the robot's height. This was done to showcase the navigation capabilities of the platforms and to test the odometry algorithms.

        % TODO add a simple nav2 costmap image herewq

        \subsubsection{3D local path planning}
        \label{section:local_path_planning}

        The local path planning in a 3D environment makes use of the Falco paper \cite{zhang2020falco}. In this paper, a method is proposed where the environment is considered deterministically known within a certain range, usually the range of the LIDAR, and probabilistically known beyond the sensor's range. This contrasts with usual methods that require an online search of a graph that needs updating at every sensor reading. The proposed method by Ji Shang et al. eliminates the need for the online search, favoring a path that maximizes the likelihood to reach the goal, instead of selecting the shortest path like traditional methods.

        %! add reference to figure
        %TODO : add a figure of the local planner paths 
        \begin{figure}[H]
            \centering
            \color{red}
            ADD LOCAL PLANNER PATHS RVIZ2
            \caption{Considered paths by the local planner}
        \end{figure}


        The local planner uses the terrain analysis mentioned in section \ref{section:terrain_analysis} to determine the navigable areas. It also subscribes to the odometry, referred to as "/state\_estimation" in the figure \ref{fig:local_planner}, and the registered scan. Its main goal is obtained from the topic "/way\_point" that is generated by the global planner. The local planner then publishes the best path to the topic "/path" that is used by the controller to move the robot, as well as all free paths considered on the topic "/free\_paths". As can be seen in figure \ref{fig:local_planner}, the local planner also makes sure to only consider paths that are within the exploration boundary if one is defined by subscribing to the topic "/exploration\_boundary".

        Setting the correct margin of inflation for the obstacles and the min/max obstacle height for the real robot was a challenge. The margin needed to be large enough to not run into walls and obstacles, but small enough so that the planner would go through tight doorways and passages. 

        
        \begin{figure}[h]
            \centering
            \resizebox{0.6\textwidth}{!}{%
            \input{graphs/local_planner.tex}
            }   
            \caption{localPlanner subscriptions and publications}
            \label{fig:local_planner}
        \end{figure}
        
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.65\textwidth]{Images/localPlannerSimCropped.png}
            \caption{Local planner in simulation, the yellow points are the considered paths, the other points are from the terrain analysis. The robot is represented as a set of axis}
            \label{fig:local_planner_sim}
        \end{figure}
        
        % TODO also add the path following node explaination
        
        
        \subsubsection{Comparison of 3D planning algorithms}

        

    \subsection{Exploration}
        \subsubsection{Metrics for exploration}
        % show how we calculate the exploration rate using the tools provided in the TARE planner
        % explain the need for a pre-recorded pointcloud of the area we wish to explore
        \subsubsection{TARE-PLANNER}
        % go in details about the process that the TARE-planner uses to explore and the motivations for the original creation of this planner (darpa sub teranean chaleneg)

        TARE \cite{tare}, is a hierarchical framework for exploring unknown 3D environment. This planner was designed for Carnegie Mellon University's participation at the 2021 DARPA subterranean challenge \cite{darpa_subterranean_challenge}. In Figure \ref{fig:tare_darpa_exploration}, the exploration of one of DARPA's environments  can be seen explored by tare planner. The planner doesn't use any prerecorded point cloud for exploration, but uses a viewpoint candidate method for exploration. The planner is able to explore large environments by building a reduced local path, while maintaining a coarse global path.

        Viewpoint candidates are generated by the planner within the local planning horizon (see Figure \ref{fig:tare_local_global}), and on valid positions of the terrain map. Each candidate is then evaluated for the amount of information it would bring to the map. This is done by evaluating the uncovered surface of the map, and evaluating the amount of those areas the viewpoint candidate would cover from the LIDAR's perspective.  

        This method also implements a hierarchical exploration strategy where the entire exploration path rather than maximizing greedily the exploration rate. In contrasts to other methods such as Motion primitive-Based exploration with path-Planner (MBP) \cite{dharmadhikari2020motion} or the Next Best View Planner (NBVP) \cite{bircher2016receding}


        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{Images/tare_darpa_exploration.jpg}
            \caption{TARE planner exploration during DARPA SubT challenge}
            \label{fig:tare_darpa_exploration}
        \end{figure}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\textwidth]{Images/tare.pdf}
            \caption{Local and global paths of the TARE planner, a dense map is conserved only for the local planning horizon, while a sparse map is kept for the global planning}
            \label{fig:tare_local_global}
        \end{figure}



        \subsubsection{MTARE-PLANNER}
        % multi agent exploration
        \subsubsection{Comparison of exploration algorithms}
        % metion the other exploration planners that are mentioned in the literature and how they compare to TARE and MTARE


    
\newpage
\section{Simulation}
    % TODO add paragraph for why simulation is needed and what we want to achieve with it

    In robotics, simulation is a key part of the development process. Unlike traditional software development, the testing of algorithms needs to be done on the physical platform, which can be time consuming and costly. Simulation allows for the testing of algorithms in a controlled environment, where the behavior of the robot can be closely mimicked. It also removes any risks associated with testing on the physical platform, such as damaging the robot or causing injury. 

    \subsection{Choosing a simulation environment}

        To test the algorithms and the coordination of the different platforms, a simulation environment was needed. The choice of the simulation environment was based on the following criteria : being able to simulate multiple platforms, being able to simulate the sensors we had on the physical platforms, and the portability of the simulation environment.


        \subsubsection{Gazebo}
            Gazebo is a well known simulation environment in the robotics community. It is widely used and has a large community. It is open source and has a lot of plugins available, for simulation sensors, motion platforms and more. As I was already familiar with it, I first looked at it to build a crude simulation of the 6 wheeled platform. Thanks to the use of ROS2 and ROS2-control for the drive train, I was able to quickly build a simulation of the platform. 

            I was also able to find a working simulation of the Livox Mid-360 LIDAR sensor we chose for every platform. The package \cite{livox_lidar_simulation_fork} was a fork of the original simulation package from Livox \cite{livox_laser_simulation} and was modified to work with the specific LIDAR we are using. 
        \subsubsection{Nvidia Isaac Sim}

            Isaac sim is a high fidelity simulation environment developed by Nvidia. This simulation environment uses a PhysX based physics engine and is able to simulate multiple platforms at once. It is also able to simulate sensors like cameras, LIDARs and IMUs. The main advantage of Isaac sim is its high fidelity and parallelization. However, this also brings a lot of complexity and the need for a RTX GPU to run it.

            The high overhead, complexity, low portability and the fact that it was not open source made me choose Gazebo over Isaac sim for the simulation of the platforms.

            In addition, the LIDAR we are using has a non-repetitive pattern, which is not currently supported by Isaac sim and would have required a lot of work to simulate.

            I did experiment with Isaac sim to simulate the quadruped platform, as it was the most complex platform to simulate and there existed a simulation of the robot in Isaac sim.
    \subsection{Simulating sensors}
        \subsubsection{LIDAR}

        \begin{figure}[H]
            \centering
            \subfigure[Gazebo simulation environment]{\includegraphics[height=0.25\textwidth]{Images/croped lidar simulation gazebo.png}}
            \subfigure[Point cloud from simulated LIDAR]{\includegraphics[height=0.25\textwidth]{Images/croped lidar simulation rviz2.png}}
            \caption{Simulated LIDAR in Gazebo and point cloud in RVIZ2}
        \end{figure}

        \subsubsection{IMU}
        \subsubsection{Camera}

        In gazebo, cameras can be simulated by modifying the robot's description and adding a sensor tag to the robot's description. The camera sensor can be configured to have a certain field of view, resolution, and clipping planes. The camera sensor can also be configured to always be on, to have a certain update rate, to visualize the image, and to publish the image to a certain topic. An example of a camera sensor configuration can be seen in listing \ref{lst:camera_sensor}.

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[style=xmlStyle, caption={Camera Sensor Configuration}, label={lst:camera_sensor}]
<sensor name="camera" type="camera">
    <camera>
        <horizontal_fov>1.047</horizontal_fov>
        <image>
            <width>320</width>
            <height>240</height>
        </image>
    </camera>
    <always_on>1</always_on>
    <update_rate>30</update_rate>
    <visualize>true</visualize>
    <topic>camera</topic>
</sensor>
\end{lstlisting}
\end{minipage}



        Similarly, a depth camera can be simulated, as we intend to add am Intel RealSense D435 to the quadruped platform. The depth camera can be configured to have a certain field of view, resolution, clipping planes, and update rate. As depth camera tend to be noisy in the depth data, the noise can be added to the depth data. An example of a depth camera sensor configuration can be seen in listing \ref{lst:depth_camera_sensor}. Here, the noise is set to be Gaussian with a mean of 0.0 and a standard deviation of 0.01. Those values were not measured but are a good starting point for the simulation. In the case of the depth camera, the clipping planes are set to 0.05 and 8.0 meters, as the depth camera is not able to see further than 8 meters. The depth camera is also configured to have a reduced resolution of just 640x480 pixels. Both color and depth views can be seen in Figure \ref{fig:cameras_simulation_view}.

\begin{minipage}{0.9\textwidth}      
\begin{lstlisting}[style=xmlStyle, caption={Depth Camera Sensor Configuration}, label={lst:depth_camera_sensor}]
<sensor name="camera" type="depth">
    <visualize>true</visualize>
    <update_rate>10</update_rate>
    <camera>
        <horizontal_fov>1.089</horizontal_fov>
        <image>
            <format>R8G8B8</format>
            <width>640</width>
            <height>480</height>
        </image>
        <clip>
            <near>0.05</near>
            <far>8.0</far>
        </clip>
    </camera>
    <plugin name="camera_controller" 
    filename="libgazebo_ros_camera.so">
    </plugin>
</sensor>
\end{lstlisting}
\end{minipage}

\begin{figure}[H]
    \centering
    \subfigure[RGB camera view]{\includegraphics[width=0.45\textwidth]{Images/rviz_rgb.png}}
    \subfigure[Depth camera view]{\includegraphics[width=0.45\textwidth]{Images/rviz_depth.png}}
    \caption{RGB and depth camera simulation view}
    \label{fig:cameras_simulation_view}
\end{figure}

    \subsubsection{Simulated world}

    As the target application of this multi agent exploration is to map a construction environment, a simulated world of an office in construction was created. The world is mainly based on the Gazebo office world by Clearpath Robotics and has two versions : one in construction with walls being built and construction equipment all over, and one where the office is constructed as can be seen in figure \ref{fig:office_world}. The figure also shows the acquired pint cloud from a stationary LIDAR in the office world. 

    \begin{figure}[H]
        \centering
        \subfigure[Office in construction point-cloud]{\includegraphics[width=0.49\textwidth]{Images/construction rviz.png}}
        \subfigure[Office constructed point-cloud]{\includegraphics[width=0.49\textwidth]{Images/office_constructed_rviz.png}}
        \subfigure[Office in construction gazebo]{\includegraphics[width=0.49\textwidth]{Images/Construction_gazebo.png}}
        \subfigure[Office constructed]{\includegraphics[width=0.49\textwidth]{Images/office_constructed_gazebo.png}}
        \caption{Office world in construction and constructed}
        \label{fig:office_world}
    \end{figure}




\section{Contributions}
    % TODO : working configuration of TARE for a GO2

    % TODO : list my contributions to open source projects


    TEST \cite{zhang2020falco}

\newpage
\section{Conclusion}
%\addcontentsline{toc}{section}{Conclusion}

    Conclusion     

\newpage
\bibliography{refs} % Entries are in the refs.bib file
\addcontentsline{toc}{section}{Bibliography}

\newpage
\addcontentsline{toc}{section}{Glossaire}
\printnoidxglossaries %glossaire, dans le fichier lexique.tex

\newpage
\addcontentsline{toc}{section}{List of figures}
\listoffigures


\newpage
\section*{Annexe}
\addcontentsline{toc}{section}{Annexe}
Uncomment input annexe when needed
%\input{annexe}



    
\end{document}